{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adc2062-2c01-4f14-a4c8-39f1dca875fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# Charger le fichier JSON\n",
    "with open('avito_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Fonction pour supprimer les clÃ©s avec des valeurs NaN\n",
    "def remove_nan_keys(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_nan_keys(v) for k, v in obj.items() \n",
    "                if not (isinstance(v, float) and math.isnan(v))}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_nan_keys(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Appliquer la suppression\n",
    "data_clean = remove_nan_keys(data)\n",
    "\n",
    "# Sauvegarder le rÃ©sultat\n",
    "with open('avito_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_clean, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da92d556-773c-45c1-9b36-d662f566f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘          SCRAPER URLs RÃ‰ELLES AVITO - MODE INDIVIDUEL            â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“‚ Chargement de vos donnÃ©es...\n",
      "âœ… DonnÃ©es chargÃ©es: 21719 annonces\n",
      "\n",
      "ğŸ‘€ AperÃ§u des donnÃ©es:\n",
      "  1. ID: 75962308 - iphone xr bon etat \n",
      "  2. ID: 75687396 - iphone 7plus \n",
      "  3. ID: 75446550 - Nokia x100 6g 128g\n",
      "\n",
      "âš™ï¸  Configuration du scraping:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tester d'abord avec 3 annonces? (o/n) [o]:  o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª MODE TEST: 3 premiÃ¨res annonces seulement\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Taille des lots (dÃ©faut: 10):  15\n",
      "DÃ©lai entre les lots en secondes (dÃ©faut: 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ CONFIGURATION FINALE:\n",
      "ğŸ“Š Annonces Ã  traiter: 3\n",
      "ğŸ“¦ Taille des lots: 15\n",
      "â±ï¸  DÃ©lai entre lots: 2 secondes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "DÃ©marrer le scraping? (o/n):  o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ DÃ‰MARRAGE DU SCRAPING...\n",
      "======================================================================\n",
      "ğŸ¯ DÃ‰BUT DU SCRAPING DES URLs (MODE INDIVIDUEL)\n",
      "ğŸ“Š Total d'annonces: 3\n",
      "ğŸ“¦ Taille des lots: 15\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Lot 1/1 (annonces 1-3)\n",
      "      âš ï¸  Erreur 422 pour 75962308ur 75962308... \n",
      "âŒ Erreur\n",
      "      ğŸ”— [2/3] RÃ©cupÃ©ration URL pour 75687396...       âš ï¸  Erreur 422 pour 75687396\n",
      "âŒ Erreur\n",
      "      ğŸ”— [3/3] RÃ©cupÃ©ration URL pour 75446550...       âš ï¸  Erreur 422 pour 75446550\n",
      "âŒ Erreur\n",
      "      ğŸ“Š RÃ©sultats lot: 0âœ… 3âŒ\n",
      "      ğŸ“ˆ Progression globale: 0âœ… 0ğŸ” 3âŒ\n",
      "\n",
      "======================================================================\n",
      "âœ¨ SCRAPING TERMINÃ‰!\n",
      "ğŸ“Š RÃ‰SULTATS FINAUX:\n",
      "   âœ… URLs trouvÃ©es: 0/3\n",
      "   ğŸ” Annonces non trouvÃ©es: 0/3\n",
      "   âŒ Erreurs: 3/3\n",
      "   ğŸ“ˆ Taux de succÃ¨s: 0.0%\n",
      "\n",
      "ğŸ’¾ RÃ©sultats sauvegardÃ©s:\n",
      "   ğŸ“„ CSV: avito_urls_results\\avito_with_real_urls_20251130_015821.csv\n",
      "   ğŸ“„ JSON: avito_urls_results\\avito_with_real_urls_20251130_015821.json\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š RAPPORT FINAL\n",
      "â±ï¸  Temps total: 2.08 secondes\n",
      "ğŸ“Š Annonces traitÃ©es: 3\n",
      "âœ… URLs rÃ©elles trouvÃ©es: 0\n",
      "\n",
      "âŒ Aucune URL trouvÃ©e. ProblÃ¨mes possibles:\n",
      "   - L'API a changÃ©\n",
      "   - Les annonces ont expirÃ©\n",
      "   - Restrictions d'accÃ¨s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION GRAPHQL\n",
    "# ============================================================================\n",
    "\n",
    "GRAPHQL_API_URL = \"https://gateway.avito.ma/graphql\"\n",
    "GRAPHQL_API_HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Origin\": \"https://www.avito.ma\",\n",
    "    \"Referer\": \"https://www.avito.ma/\",\n",
    "}\n",
    "\n",
    "# REQUÃŠTE GRAPHQL POUR RÃ‰CUPÃ‰RER L'URL INDIVIDUELLE\n",
    "GRAPHQL_URL_QUERY = \"\"\"\n",
    "query getAdDetails($adId: ID!) {\n",
    "  getAd(adId: $adId) {\n",
    "    ... on PublishedAd {\n",
    "      adId\n",
    "      title\n",
    "      url\n",
    "      category {\n",
    "        name\n",
    "        parent {\n",
    "          name\n",
    "        }\n",
    "      }\n",
    "      price {\n",
    "        withCurrency\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTIONS CORRIGÃ‰ES\n",
    "# ============================================================================\n",
    "\n",
    "def fetch_single_url_via_graphql(ad_id, retry=3, delay=1):\n",
    "    \"\"\"RÃ©cupÃ¨re l'URL rÃ©elle pour une seule annonce via GraphQL\"\"\"\n",
    "    variables = {\"adId\": str(ad_id)}\n",
    "    \n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                GRAPHQL_API_URL,\n",
    "                headers=GRAPHQL_API_HEADERS,\n",
    "                json={\"query\": GRAPHQL_URL_QUERY, \"variables\": variables},\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Debug: afficher la rÃ©ponse pour les premiers IDs\n",
    "                if ad_id in [75962308, 75687396, 75446550]:  # Vos premiers IDs\n",
    "                    print(f\"      ğŸ” DEBUG RÃ©ponse pour {ad_id}: {json.dumps(data, ensure_ascii=False)[:300]}\")\n",
    "                \n",
    "                if data.get(\"data\") and data[\"data\"].get(\"getAd\"):\n",
    "                    ad_data = data[\"data\"][\"getAd\"]\n",
    "                    url = ad_data.get(\"url\")\n",
    "                    if url:\n",
    "                        return url\n",
    "                    else:\n",
    "                        print(f\"      âš ï¸  Pas d'URL dans la rÃ©ponse pour {ad_id}\")\n",
    "                elif data.get(\"errors\"):\n",
    "                    error_msg = data['errors'][0].get('message', 'Unknown error')\n",
    "                    print(f\"      âŒ Erreur API pour {ad_id}: {error_msg}\")\n",
    "                    \n",
    "                    # Si l'erreur indique que l'annonce n'existe pas\n",
    "                    if \"not found\" in error_msg.lower() or \"does not exist\" in error_msg.lower():\n",
    "                        return \"NOT_FOUND\"\n",
    "                \n",
    "            elif response.status_code == 429:  # Too Many Requests\n",
    "                print(f\"      âš ï¸  Rate limit (429), pause de 30s...\")\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "            elif response.status_code == 422:  # Unprocessable Entity\n",
    "                print(f\"      âš ï¸  Erreur 422 pour {ad_id}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"      âŒ Statut {response.status_code} pour {ad_id}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"      âš ï¸  Erreur rÃ©seau (tentative {attempt + 1}/{retry}): {e}\")\n",
    "        \n",
    "        if attempt < retry - 1:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_urls_individual(existing_data, batch_size=10, delay_between_batches=2):\n",
    "    \"\"\"\n",
    "    Scrape les URLs rÃ©elles une par une via l'API GraphQL\n",
    "    \"\"\"\n",
    "    if isinstance(existing_data, list):\n",
    "        df = pd.DataFrame(existing_data)\n",
    "    else:\n",
    "        df = existing_data.copy()\n",
    "    \n",
    "    print(f\"ğŸ¯ DÃ‰BUT DU SCRAPING DES URLs (MODE INDIVIDUEL)\")\n",
    "    print(f\"ğŸ“Š Total d'annonces: {len(df)}\")\n",
    "    print(f\"ğŸ“¦ Taille des lots: {batch_size}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    urls_found = 0\n",
    "    not_found_count = 0\n",
    "    error_count = 0\n",
    "    total_ads = len(df)\n",
    "    \n",
    "    # Ajouter les colonnes pour les rÃ©sultats\n",
    "    df['scraped_url'] = None\n",
    "    df['url_status'] = 'pending'\n",
    "    \n",
    "    # Traiter par lots\n",
    "    for i in range(0, total_ads, batch_size):\n",
    "        batch_end = min(i + batch_size, total_ads)\n",
    "        current_batch_size = batch_end - i\n",
    "        batch_num = i // batch_size + 1\n",
    "        total_batches = (total_ads + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"\\nğŸ”„ Lot {batch_num}/{total_batches} (annonces {i+1}-{batch_end})\")\n",
    "        \n",
    "        batch_success = 0\n",
    "        batch_errors = 0\n",
    "        \n",
    "        for j in range(i, batch_end):\n",
    "            ad_id = df.at[j, 'adId']\n",
    "            current_num = j + 1\n",
    "            \n",
    "            print(f\"      ğŸ”— [{current_num}/{total_ads}] RÃ©cupÃ©ration URL pour {ad_id}...\", end=\" \")\n",
    "            \n",
    "            url = fetch_single_url_via_graphql(ad_id)\n",
    "            \n",
    "            if url:\n",
    "                if url == \"NOT_FOUND\":\n",
    "                    df.at[j, 'scraped_url'] = None\n",
    "                    df.at[j, 'url_status'] = 'not_found'\n",
    "                    not_found_count += 1\n",
    "                    print(\"âŒ Annonce non trouvÃ©e\")\n",
    "                else:\n",
    "                    df.at[j, 'scraped_url'] = url\n",
    "                    df.at[j, 'url_status'] = 'found'\n",
    "                    urls_found += 1\n",
    "                    batch_success += 1\n",
    "                    print(\"âœ… TrouvÃ©e\")\n",
    "            else:\n",
    "                df.at[j, 'scraped_url'] = None\n",
    "                df.at[j, 'url_status'] = 'error'\n",
    "                error_count += 1\n",
    "                batch_errors += 1\n",
    "                print(\"âŒ Erreur\")\n",
    "            \n",
    "            # Petite pause entre les requÃªtes\n",
    "            if j < batch_end - 1:\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        print(f\"      ğŸ“Š RÃ©sultats lot: {batch_success}âœ… {batch_errors}âŒ\")\n",
    "        print(f\"      ğŸ“ˆ Progression globale: {urls_found}âœ… {not_found_count}ğŸ” {error_count}âŒ\")\n",
    "        \n",
    "        # Sauvegarde temporaire tous les 10 lots\n",
    "        if batch_num % 10 == 0:\n",
    "            timestamp = datetime.now().strftime('%H%M%S')\n",
    "            temp_file = f\"temp_urls_progress_{timestamp}.json\"\n",
    "            df.to_json(temp_file, orient='records', indent=2, force_ascii=False)\n",
    "            print(f\"      ğŸ’¾ Sauvegarde temporaire: {temp_file}\")\n",
    "        \n",
    "        # Pause entre les lots\n",
    "        if batch_end < total_ads:\n",
    "            print(f\"      â¸ï¸  Pause de {delay_between_batches} secondes...\")\n",
    "            time.sleep(delay_between_batches)\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"âœ¨ SCRAPING TERMINÃ‰!\")\n",
    "    print(f\"ğŸ“Š RÃ‰SULTATS FINAUX:\")\n",
    "    print(f\"   âœ… URLs trouvÃ©es: {urls_found}/{total_ads}\")\n",
    "    print(f\"   ğŸ” Annonces non trouvÃ©es: {not_found_count}/{total_ads}\")\n",
    "    print(f\"   âŒ Erreurs: {error_count}/{total_ads}\")\n",
    "    print(f\"   ğŸ“ˆ Taux de succÃ¨s: {(urls_found/total_ads)*100:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT PRINCIPAL CORRIGÃ‰\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"â•‘          SCRAPER URLs RÃ‰ELLES AVITO - MODE INDIVIDUEL            â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    \n",
    "    # Charger vos donnÃ©es\n",
    "    print(\"\\nğŸ“‚ Chargement de vos donnÃ©es...\")\n",
    "    try:\n",
    "        with open('avito_data.json', 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "        print(f\"âœ… DonnÃ©es chargÃ©es: {len(existing_data)} annonces\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur chargement: {e}\")\n",
    "        return\n",
    "    \n",
    "    # AperÃ§u\n",
    "    print(f\"\\nğŸ‘€ AperÃ§u des donnÃ©es:\")\n",
    "    for i, ad in enumerate(existing_data[:3]):\n",
    "        print(f\"  {i+1}. ID: {ad['adId']} - {ad['title']}\")\n",
    "    \n",
    "    # Configuration\n",
    "    print(f\"\\nâš™ï¸  Configuration du scraping:\")\n",
    "    \n",
    "    # Tester d'abord avec un petit Ã©chantillon\n",
    "    test_sample = input(\"Tester d'abord avec 3 annonces? (o/n) [o]: \").strip().lower() != 'n'\n",
    "    \n",
    "    if test_sample:\n",
    "        sample_data = existing_data[:3]\n",
    "        print(\"ğŸ§ª MODE TEST: 3 premiÃ¨res annonces seulement\")\n",
    "    else:\n",
    "        use_sample = input(\"Utiliser un Ã©chantillon? (o/n) [n]: \").strip().lower() == 'o'\n",
    "        if use_sample:\n",
    "            sample_size = input(f\"Taille Ã©chantillon (dÃ©faut: 100): \").strip()\n",
    "            sample_size = int(sample_size) if sample_size else 100\n",
    "            sample_data = existing_data[:sample_size]\n",
    "        else:\n",
    "            sample_data = existing_data\n",
    "    \n",
    "    batch_size = input(\"Taille des lots (dÃ©faut: 10): \").strip()\n",
    "    batch_size = int(batch_size) if batch_size else 10\n",
    "    \n",
    "    delay = input(\"DÃ©lai entre les lots en secondes (dÃ©faut: 2): \").strip()\n",
    "    delay = int(delay) if delay else 2\n",
    "    \n",
    "    print(f\"\\nğŸ¯ CONFIGURATION FINALE:\")\n",
    "    print(f\"ğŸ“Š Annonces Ã  traiter: {len(sample_data)}\")\n",
    "    print(f\"ğŸ“¦ Taille des lots: {batch_size}\")\n",
    "    print(f\"â±ï¸  DÃ©lai entre lots: {delay} secondes\")\n",
    "    \n",
    "    confirmation = input(\"\\nDÃ©marrer le scraping? (o/n): \").strip().lower()\n",
    "    if confirmation != 'o':\n",
    "        print(\"âŒ AnnulÃ©\")\n",
    "        return\n",
    "    \n",
    "    # Lancer le scraping\n",
    "    print(f\"\\nğŸš€ DÃ‰MARRAGE DU SCRAPING...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    results_df = scrape_urls_individual(\n",
    "        sample_data, \n",
    "        batch_size=batch_size, \n",
    "        delay_between_batches=delay\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Sauvegarder les rÃ©sultats\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = Path(\"avito_urls_results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Fichier CSV\n",
    "    csv_path = output_dir / f\"avito_with_real_urls_{timestamp}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Fichier JSON\n",
    "    json_path = output_dir / f\"avito_with_real_urls_{timestamp}.json\"\n",
    "    results_df.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ RÃ©sultats sauvegardÃ©s:\")\n",
    "    print(f\"   ğŸ“„ CSV: {csv_path}\")\n",
    "    print(f\"   ğŸ“„ JSON: {json_path}\")\n",
    "    \n",
    "    # Afficher les statistiques finales\n",
    "    urls_found = len(results_df[results_df['url_status'] == 'found'])\n",
    "    total_processed = len(results_df)\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"ğŸ“Š RAPPORT FINAL\")\n",
    "    print(f\"â±ï¸  Temps total: {duration:.2f} secondes\")\n",
    "    print(f\"ğŸ“Š Annonces traitÃ©es: {total_processed}\")\n",
    "    print(f\"âœ… URLs rÃ©elles trouvÃ©es: {urls_found}\")\n",
    "    \n",
    "    if urls_found > 0:\n",
    "        print(f\"ğŸ“ˆ Taux de succÃ¨s: {(urls_found/total_processed)*100:.1f}%\")\n",
    "        print(f\"\\nğŸ‘€ EXEMPLES D'URLs RÃ‰ELLES TROUVÃ‰ES:\")\n",
    "        success_data = results_df[results_df['url_status'] == 'found'].head(3)\n",
    "        for _, row in success_data.iterrows():\n",
    "            print(f\"  ğŸ”— {row['adId']}: {row['scraped_url']}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Aucune URL trouvÃ©e. ProblÃ¨mes possibles:\")\n",
    "        print(\"   - L'API a changÃ©\")\n",
    "        print(\"   - Les annonces ont expirÃ©\")\n",
    "        print(\"   - Restrictions d'accÃ¨s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239ad5c-b49a-4550-a191-b3209bed04cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
